{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0254f-5b47-49aa-9d13-43d0b1aa000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize hand detector and classifier(pre trainde model)\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"C:/Users/RAVI/Downloads/Sign language Project/Model/keras_model.h5\", \"C:/Users/RAVI/Downloads/Sign language Project/Model/labels.txt\")\n",
    "\n",
    "# Constants\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "labels = [\"Hello\", \"iloveyou\", \"Okay\", \"Yes\", \"No\", \"Thankyou\"]\n",
    "\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Make a copy of the original frame\n",
    "    imgOutput = img.copy()\n",
    "\n",
    "    # Find hands in the frame\n",
    "    hands, img = detector.findHands(img)\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        # Create a white image\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "        # Crop and resize the hand region\n",
    "        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "        if imgCrop.size == 0:\n",
    "            print(\"Warning: Empty cropped image.\")\n",
    "            continue\n",
    "\n",
    "        aspectRatio = h / w\n",
    "\n",
    "        if aspectRatio > 1:\n",
    "            k = imgSize / h\n",
    "            wCal = math.ceil(k * w)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            k = imgSize / w\n",
    "            hCal = math.ceil(k * h)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "        # Get prediction from the classifier\n",
    "        prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "        print(prediction, index)\n",
    "\n",
    "        # Display labels and bounding box\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset - 70), (x - offset + 400, y - offset + 60 - 50), (0, 255, 0),\n",
    "                      cv2.FILLED)\n",
    "        cv2.putText(imgOutput, labels[index], (x, y - 30), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 0), 2)\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset), (0, 255, 0), 4)\n",
    "\n",
    "        # Display cropped and resized images\n",
    "        cv2.imshow('ImageCrop', imgCrop)\n",
    "        cv2.imshow('ImageWhite', imgWhite)\n",
    "\n",
    "    # Display original image with annotations\n",
    "    cv2.imshow('Image', imgOutput)\n",
    "\n",
    "    # Check for exit key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the camera and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ec256-cd26-48c4-a939-907814ea5b7b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb8ebe3-961f-4397-a50b-2140e4ae1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ed328-7177-4003-99fd-999030a82102",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689786c8-7948-4c22-8577-d6f08fddc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade8923-eb69-4356-8a32-597930774376",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631e351a-43cd-4ce7-b702-fd1391f28494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifically, the HandDetector module is responsible for finding hands in the frame, which involves preprocessing steps like converting the image to grayscale, applying filters, and extracting hand regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17ff7b-f070-4841-b869-69aa86c08898",
   "metadata": {},
   "outputs": [],
   "source": [
    "The first script is for collecting and saving hand gesture images.\n",
    "The second script detects hand gestures in real-time, classifies them using a pre-trained model, and displays the results.\n",
    "Both scripts utilize OpenCV for image capture and display, and cvzone for hand detection and gesture classification.\n",
    "Classifier: Loads a pre-trained model and corresponding labels for gesture classification using Classifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
